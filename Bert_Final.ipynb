{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa781ac5d2b345afabfc9277e6f39189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35ff0edb434b40e98186e44c483e7b6e",
              "IPY_MODEL_e02fe8bb50e94875bc903ce654c8e0c2",
              "IPY_MODEL_5bbd12de93544fc182b56daa0f670d91"
            ],
            "layout": "IPY_MODEL_9666eda3d696465cb85df4493c70c6b4"
          }
        },
        "35ff0edb434b40e98186e44c483e7b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569a13b6fb4e4ac8b6f5d03ca28a8a91",
            "placeholder": "​",
            "style": "IPY_MODEL_e4c5cab7829c4bd0a537ae9b5a274af3",
            "value": "100%"
          }
        },
        "e02fe8bb50e94875bc903ce654c8e0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061cfb6d04bb4322a2172a8ab23f40cf",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9881bd0dcac747c088bb6aba560eb512",
            "value": 100
          }
        },
        "5bbd12de93544fc182b56daa0f670d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aad8b847cf94bbeae33fc812f924b49",
            "placeholder": "​",
            "style": "IPY_MODEL_6cddf42548094be380523ea65ae9d792",
            "value": " 100/100 [00:00&lt;00:00, 692.93ex/s]"
          }
        },
        "9666eda3d696465cb85df4493c70c6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569a13b6fb4e4ac8b6f5d03ca28a8a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c5cab7829c4bd0a537ae9b5a274af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "061cfb6d04bb4322a2172a8ab23f40cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9881bd0dcac747c088bb6aba560eb512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8aad8b847cf94bbeae33fc812f924b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cddf42548094be380523ea65ae9d792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT Question Answering Model**"
      ],
      "metadata": {
        "id": "0pBgrPCyQGy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Mount drive**"
      ],
      "metadata": {
        "id": "lGAgQSwhQLzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzEg0SO0by3o",
        "outputId": "084ff18f-909e-4671-c984-20c597f19d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Move inside directory**"
      ],
      "metadata": {
        "id": "jIF82bxuQOdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/chaii-hindi-and-tamil-question-answering\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIf_IzlrTAxB",
        "outputId": "bfee469a-7c19-4ab8-bec9-b70e091ea6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/chaii-hindi-and-tamil-question-answering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Install required packages**"
      ],
      "metadata": {
        "id": "2QFGtLKnQTrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install torchtext==0.10.0\n",
        "!pip install Dataset\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install indic-nlp-library\n",
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEIMY1Wmk1Gj",
        "outputId": "fd82db6c-bf35-475e-b484-59ba6e327f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Dataset\n",
            "  Downloading dataset-1.5.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting alembic>=0.6.2\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting banal>=1.0.1\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from Dataset) (1.4.42)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->Dataset) (4.13.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->Dataset) (5.10.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.2->Dataset) (1.1.3.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->Dataset) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->Dataset) (3.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=0.6.2->Dataset) (2.0.1)\n",
            "Installing collected packages: Mako, banal, alembic, Dataset\n",
            "Successfully installed Dataset-1.5.2 Mako-1.2.3 alembic-1.8.1 banal-1.0.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 28.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 25.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 96.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 80.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 100.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 99.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.9.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 92.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.6.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 63.0 MB/s \n",
            "\u001b[?25hCollecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.2-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.3.5)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.81 morfessor-2.0.6 sphinx-argparse-0.3.2 sphinx-rtd-theme-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.9.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.9.1\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Installing collected packages: soupsieve, beautifulsoup4, deep-translator\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.11.1 deep-translator-1.9.0 soupsieve-2.3.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Impor tlibraries**"
      ],
      "metadata": {
        "id": "KsTT2ifGQapw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGN375z-SScK"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from transformers import default_data_collator, Trainer\n",
        "from transformers import BertForQuestionAnswering, AutoTokenizer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data collator object**"
      ],
      "metadata": {
        "id": "X_2kIdqwQdJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = default_data_collator"
      ],
      "metadata": {
        "id": "0IJUT9WoM8mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"mrm8488/bert-multi-cased-finetuned-xquadv1\""
      ],
      "metadata": {
        "id": "LC8om6IR4O5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Load pretrained model**"
      ],
      "metadata": {
        "id": "JcDXWx98Qg_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = BertForQuestionAnswering.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "heTh2XQbTYBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_style = tokenizer.padding_side == \"right\""
      ],
      "metadata": {
        "id": "Bl50X83lTg8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Read dataset**"
      ],
      "metadata": {
        "id": "R54Hfl3WQkcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\", index_col=0)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwdLtpqMeyhb",
        "outputId": "a164a665-02ad-474e-bd10-ed69070608b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1114, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for null values**"
      ],
      "metadata": {
        "id": "qA5Cj3epWfu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for null values\n",
        "train.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUO-bzVlV7_6",
        "outputId": "2a1ef163-2baf-4be0-9758-eda7e48cc550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "context         0\n",
              "question        0\n",
              "answer_text     0\n",
              "answer_start    0\n",
              "language        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data cleaning**"
      ],
      "metadata": {
        "id": "ZfJz99rVQnRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_space(text):\n",
        "  tex = text.lstrip() \n",
        "  return text"
      ],
      "metadata": {
        "id": "kG_Y0EVBW-eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['question'] = train['question'].apply(remove_space)"
      ],
      "metadata": {
        "id": "z6roylf0aF71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Drop null values if there are any**"
      ],
      "metadata": {
        "id": "NHVLA2oEWqrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop null values and reset index\n",
        "train = train.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "oqFH2UfIWnrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Map answers to dictionary**\n",
        "- **Find end answer position**"
      ],
      "metadata": {
        "id": "tBoGq9fzQsuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_answers_todict(col):\n",
        "    s_position = col[0]\n",
        "    answer_text = col[1]\n",
        "    start_pos = col[0]\n",
        "    e_position = start_pos + len(answer_text)\n",
        "    answer_dict = { 'answer_start': [s_position], 'answer_text': [answer_text],'answer_end': [e_position]}\n",
        "    return answer_dict"
      ],
      "metadata": {
        "id": "0F-8mkFF9-Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Train, validation and test split**"
      ],
      "metadata": {
        "id": "fQEyqtj5RZcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train, test and validation sets\n",
        "train_data = train.iloc[:914].reset_index(drop=True)\n",
        "test_data = train.iloc[914:1014].reset_index(drop=True)\n",
        "val_data = train.iloc[1014:].reset_index(drop=True)\n",
        "print(\"Train dataset size: \", train_data.shape)\n",
        "print(\"Test dataset size: \", test_data.shape)\n",
        "print(\"Validation dataset size: \", val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHA6bpZjbFn9",
        "outputId": "14a2d1a2-97f5-44d5-f8d4-58006c7a81d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size:  (914, 5)\n",
            "Test dataset size:  (100, 5)\n",
            "Validation dataset size:  (100, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['answers'] = train_data[['answer_start', 'answer_text']].apply(map_answers_todict, axis=1)\n",
        "test_data['answers'] = test_data[['answer_start', 'answer_text']].apply(map_answers_todict, axis=1)\n",
        "val_data['answers'] = val_data[['answer_start', 'answer_text']].apply(map_answers_todict, axis=1)"
      ],
      "metadata": {
        "id": "eB0ReotYbwUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3KKVH0GSYlk1",
        "outputId": "bd4ce02f-287f-45e7-ec7c-31a86a6bba9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context  \\\n",
              "0  ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...   \n",
              "1  காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...   \n",
              "\n",
              "                               question answer_text  answer_start language  \\\n",
              "0  மனித உடலில் எத்தனை எலும்புகள் உள்ளன?         206            53    tamil   \n",
              "1            காளிதாசன் எங்கு பிறந்தார்?  காசுமீரில்          2358    tamil   \n",
              "\n",
              "                                             answers  \n",
              "0  {'answer_start': [53], 'answer_text': ['206'],...  \n",
              "1  {'answer_start': [2358], 'answer_text': ['காசு...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1cbf0d0-83d0-4887-87d1-4b19b40e21d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...</td>\n",
              "      <td>மனித உடலில் எத்தனை எலும்புகள் உள்ளன?</td>\n",
              "      <td>206</td>\n",
              "      <td>53</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [53], 'answer_text': ['206'],...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...</td>\n",
              "      <td>காளிதாசன் எங்கு பிறந்தார்?</td>\n",
              "      <td>காசுமீரில்</td>\n",
              "      <td>2358</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [2358], 'answer_text': ['காசு...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1cbf0d0-83d0-4887-87d1-4b19b40e21d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1cbf0d0-83d0-4887-87d1-4b19b40e21d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1cbf0d0-83d0-4887-87d1-4b19b40e21d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Btu05myzYrjJ",
        "outputId": "dea6ce7b-c6bd-472a-ad46-360aa5e2622c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context  \\\n",
              "0  राजधानी:    बगदाद\\nजनसंख्या:    30,39 9, 572 (...   \n",
              "\n",
              "                      question         answer_text  answer_start language  \\\n",
              "0  ईराक का क्षेत्रफल कितना है?  16 9, 250 वर्ग मील            76    hindi   \n",
              "\n",
              "                                             answers  \n",
              "0  {'answer_start': [76], 'answer_text': ['16 9, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cde8e07-5a93-4974-9dfa-74c45cda0e65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>राजधानी:    बगदाद\\nजनसंख्या:    30,39 9, 572 (...</td>\n",
              "      <td>ईराक का क्षेत्रफल कितना है?</td>\n",
              "      <td>16 9, 250 वर्ग मील</td>\n",
              "      <td>76</td>\n",
              "      <td>hindi</td>\n",
              "      <td>{'answer_start': [76], 'answer_text': ['16 9, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cde8e07-5a93-4974-9dfa-74c45cda0e65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cde8e07-5a93-4974-9dfa-74c45cda0e65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cde8e07-5a93-4974-9dfa-74c45cda0e65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "5izTNzahYxIO",
        "outputId": "ee69268a-55f2-4480-fc70-fa39823ac224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             context  \\\n",
              "0  अमिताभ बच्चन (जन्म-११ अक्टूबर, १९४२) बॉलीवुड क...   \n",
              "1  यह जेल अंडमान निकोबार द्वीप की राजधानी पोर्ट ब...   \n",
              "\n",
              "                               question           answer_text  answer_start  \\\n",
              "0  अमिताभ बच्चन के पिता का नाम क्या था?  डॉ॰ हरिवंश राय बच्चन          1084   \n",
              "1       काला पानी जेल कहाँ पर स्थित है?          पोर्ट ब्लेयर            39   \n",
              "\n",
              "  language                                            answers  \n",
              "0    hindi  {'answer_start': [1084], 'answer_text': ['डॉ॰ ...  \n",
              "1    hindi  {'answer_start': [39], 'answer_text': ['पोर्ट ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6867276f-c714-4d96-a03b-83e1f843178e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अमिताभ बच्चन (जन्म-११ अक्टूबर, १९४२) बॉलीवुड क...</td>\n",
              "      <td>अमिताभ बच्चन के पिता का नाम क्या था?</td>\n",
              "      <td>डॉ॰ हरिवंश राय बच्चन</td>\n",
              "      <td>1084</td>\n",
              "      <td>hindi</td>\n",
              "      <td>{'answer_start': [1084], 'answer_text': ['डॉ॰ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>यह जेल अंडमान निकोबार द्वीप की राजधानी पोर्ट ब...</td>\n",
              "      <td>काला पानी जेल कहाँ पर स्थित है?</td>\n",
              "      <td>पोर्ट ब्लेयर</td>\n",
              "      <td>39</td>\n",
              "      <td>hindi</td>\n",
              "      <td>{'answer_start': [39], 'answer_text': ['पोर्ट ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6867276f-c714-4d96-a03b-83e1f843178e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6867276f-c714-4d96-a03b-83e1f843178e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6867276f-c714-4d96-a03b-83e1f843178e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Data tokenization** "
      ],
      "metadata": {
        "id": "KQxSiznVR0nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DataTokenization(Dataset):\n",
        "     # initialize all required parameters in initialization function\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        text_max_length: int\n",
        "        ):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.text_max_length = text_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        tokenized_text = tokenizer(\n",
        "        data_row[\"question\" if padding_style  else \"context\"], data_row[\"context\" if padding_style  else \"question\"],\n",
        "        padding = \"max_length\", truncation = \"only_second\" if padding_style  else \"only_first\",\n",
        "        max_length = self.text_max_length, return_attention_mask=True,\n",
        "        add_special_tokens=True, return_offsets_mapping=True, return_tensors=None)\n",
        "        # extract offset mapping from tokenized dictionary\n",
        "        offset_tokens = tokenized_text[\"offset_mapping\"]\n",
        "        tokenized_text[\"start_positions\"] = []\n",
        "        tokenized_text[\"end_positions\"] = []\n",
        "        # extract input ids from tokenized dictionary\n",
        "        input_ids = tokenized_text[\"input_ids\"]\n",
        "        tokens_style = tokenized_text.sequence_ids()\n",
        "        answers = data_row[\"answers\"]\n",
        "        # check if answer is not in the start of context\n",
        "        if len(answers[\"answer_start\"]) != 0:\n",
        "            loop_begin = 0\n",
        "            ans_begin = answers[\"answer_start\"][0]\n",
        "            ans_end = answers[\"answer_end\"][0]\n",
        "            while tokens_style[loop_begin] != (1 if padding_style  else 0):\n",
        "              loop_begin += 1\n",
        "            loop_end = len(input_ids) - 1\n",
        "             # update answer end position \n",
        "            while tokens_style[loop_end] != (1 if padding_style  else 0):\n",
        "              loop_end -= 1\n",
        "            if not (offset_tokens[loop_begin][0] <= ans_begin and offset_tokens[loop_end][1] >= ans_end):\n",
        "                tokenized_text[\"start_positions\"].append(0)\n",
        "                tokenized_text[\"end_positions\"].append(0)\n",
        "            else:\n",
        "                while loop_begin < len(offset_tokens) and offset_tokens[loop_begin][0] <= ans_begin:\n",
        "                    loop_begin += 1\n",
        "                tokenized_text[\"start_positions\"].append(loop_begin - 1)\n",
        "                # if the answer exists in the context iterate to update end positions\n",
        "                while offset_tokens[loop_end][1] >= ans_end:\n",
        "                    loop_end -= 1\n",
        "                tokenized_text[\"end_positions\"].append(loop_end + 1)\n",
        "        else:\n",
        "            tokenized_text[\"start_positions\"].append(0)\n",
        "            tokenized_text[\"end_positions\"].append(0)\n",
        "        \n",
        "        # make dictionary of all extracted ids and return\n",
        "        return dict(\n",
        "            input_ids = tokenized_text['input_ids'],\n",
        "            attention_mask = tokenized_text['attention_mask'],\n",
        "            start_positions = tokenized_text['start_positions'],\n",
        "            end_positions = tokenized_text['end_positions']\n",
        "            )"
      ],
      "metadata": {
        "id": "mAF080ApDz4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataTokenization(train_data, tokenizer, 384)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRQAXrI3YTSH",
        "outputId": "a258e009-d7cd-4b46-c128-58c9730182e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DataTokenization at 0x7f932c748a50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "train_dataset = list(DataTokenization(train_data, tokenizer, 384))\n",
        "val_dataset = list(DataTokenization(val_data, tokenizer, 384))"
      ],
      "metadata": {
        "id": "HgqUhqHqmjL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPaQ1pjFYVrA",
        "outputId": "06e29075-4baa-4405-9ca7-a8bb06f7e1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': [0,\n",
              "   69535,\n",
              "   81049,\n",
              "   37368,\n",
              "   153264,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   145615,\n",
              "   32,\n",
              "   2,\n",
              "   2,\n",
              "   3219,\n",
              "   224013,\n",
              "   124335,\n",
              "   5966,\n",
              "   69535,\n",
              "   4930,\n",
              "   74149,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   182394,\n",
              "   3686,\n",
              "   51833,\n",
              "   57210,\n",
              "   101912,\n",
              "   15,\n",
              "   6161,\n",
              "   2912,\n",
              "   70597,\n",
              "   52989,\n",
              "   21883,\n",
              "   102080,\n",
              "   54512,\n",
              "   91585,\n",
              "   1962,\n",
              "   212933,\n",
              "   18599,\n",
              "   16242,\n",
              "   94236,\n",
              "   16,\n",
              "   198236,\n",
              "   29160,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   173139,\n",
              "   23618,\n",
              "   72817,\n",
              "   5,\n",
              "   5894,\n",
              "   198236,\n",
              "   81049,\n",
              "   37334,\n",
              "   144257,\n",
              "   7827,\n",
              "   82890,\n",
              "   84853,\n",
              "   80517,\n",
              "   114452,\n",
              "   232094,\n",
              "   3686,\n",
              "   17984,\n",
              "   11830,\n",
              "   62001,\n",
              "   182394,\n",
              "   4167,\n",
              "   5,\n",
              "   203312,\n",
              "   10753,\n",
              "   50667,\n",
              "   2650,\n",
              "   4,\n",
              "   45303,\n",
              "   1962,\n",
              "   163062,\n",
              "   198236,\n",
              "   29160,\n",
              "   176030,\n",
              "   15453,\n",
              "   4,\n",
              "   3219,\n",
              "   171093,\n",
              "   5944,\n",
              "   2650,\n",
              "   8120,\n",
              "   10175,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   2650,\n",
              "   24183,\n",
              "   5638,\n",
              "   14861,\n",
              "   16,\n",
              "   56735,\n",
              "   3219,\n",
              "   171093,\n",
              "   5944,\n",
              "   2650,\n",
              "   12009,\n",
              "   145578,\n",
              "   10832,\n",
              "   2802,\n",
              "   2650,\n",
              "   26873,\n",
              "   52989,\n",
              "   21883,\n",
              "   53336,\n",
              "   26415,\n",
              "   38640,\n",
              "   31067,\n",
              "   74,\n",
              "   105457,\n",
              "   5966,\n",
              "   22050,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   202342,\n",
              "   59386,\n",
              "   12095,\n",
              "   52989,\n",
              "   13070,\n",
              "   2650,\n",
              "   1962,\n",
              "   39311,\n",
              "   9654,\n",
              "   37964,\n",
              "   18806,\n",
              "   4,\n",
              "   196586,\n",
              "   105457,\n",
              "   5966,\n",
              "   22467,\n",
              "   78876,\n",
              "   52989,\n",
              "   21883,\n",
              "   74,\n",
              "   102080,\n",
              "   6896,\n",
              "   20,\n",
              "   21162,\n",
              "   14233,\n",
              "   94097,\n",
              "   4864,\n",
              "   12152,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   210828,\n",
              "   1381,\n",
              "   198236,\n",
              "   2782,\n",
              "   191297,\n",
              "   10832,\n",
              "   2802,\n",
              "   2650,\n",
              "   26873,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   3912,\n",
              "   59987,\n",
              "   1962,\n",
              "   212933,\n",
              "   26415,\n",
              "   20567,\n",
              "   5,\n",
              "   69535,\n",
              "   9696,\n",
              "   184936,\n",
              "   28258,\n",
              "   89933,\n",
              "   1039,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   15,\n",
              "   10753,\n",
              "   2802,\n",
              "   6149,\n",
              "   11674,\n",
              "   16043,\n",
              "   26873,\n",
              "   2913,\n",
              "   21883,\n",
              "   202342,\n",
              "   137010,\n",
              "   16,\n",
              "   145615,\n",
              "   74,\n",
              "   164359,\n",
              "   12095,\n",
              "   11993,\n",
              "   181576,\n",
              "   87783,\n",
              "   35186,\n",
              "   15,\n",
              "   15182,\n",
              "   53208,\n",
              "   16,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   91585,\n",
              "   11772,\n",
              "   616,\n",
              "   71987,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   91585,\n",
              "   11772,\n",
              "   15,\n",
              "   1021,\n",
              "   26136,\n",
              "   32881,\n",
              "   7,\n",
              "   16,\n",
              "   73417,\n",
              "   73949,\n",
              "   163493,\n",
              "   5,\n",
              "   15,\n",
              "   4875,\n",
              "   5427,\n",
              "   3770,\n",
              "   136259,\n",
              "   1629,\n",
              "   88115,\n",
              "   2650,\n",
              "   230988,\n",
              "   112578,\n",
              "   53336,\n",
              "   4167,\n",
              "   136259,\n",
              "   173139,\n",
              "   6,\n",
              "   79464,\n",
              "   2798,\n",
              "   143825,\n",
              "   5,\n",
              "   16,\n",
              "   181576,\n",
              "   87783,\n",
              "   35186,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   10021,\n",
              "   106,\n",
              "   190707,\n",
              "   4875,\n",
              "   128236,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   20549,\n",
              "   289,\n",
              "   32881,\n",
              "   16,\n",
              "   116,\n",
              "   14184,\n",
              "   3937,\n",
              "   145181,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   24980,\n",
              "   13,\n",
              "   1803,\n",
              "   32881,\n",
              "   16,\n",
              "   1737,\n",
              "   138,\n",
              "   116180,\n",
              "   17056,\n",
              "   3686,\n",
              "   4875,\n",
              "   128236,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   99736,\n",
              "   289,\n",
              "   32881,\n",
              "   16,\n",
              "   1737,\n",
              "   201,\n",
              "   22262,\n",
              "   95344,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   6652,\n",
              "   88354,\n",
              "   289,\n",
              "   32881,\n",
              "   16,\n",
              "   6001,\n",
              "   6343,\n",
              "   8850,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   7,\n",
              "   88322,\n",
              "   48899,\n",
              "   32881,\n",
              "   16,\n",
              "   60070,\n",
              "   12784,\n",
              "   2782,\n",
              "   35424,\n",
              "   26873,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   12421,\n",
              "   432,\n",
              "   532,\n",
              "   32881,\n",
              "   16,\n",
              "   71987,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   1629,\n",
              "   31203,\n",
              "   361,\n",
              "   145578,\n",
              "   51153,\n",
              "   18805,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   12018,\n",
              "   28236,\n",
              "   16,\n",
              "   305,\n",
              "   177292,\n",
              "   95424,\n",
              "   18805,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   24084,\n",
              "   2298,\n",
              "   16,\n",
              "   1737,\n",
              "   2690,\n",
              "   42353,\n",
              "   78876,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   16917,\n",
              "   10325,\n",
              "   32881,\n",
              "   16,\n",
              "   1737,\n",
              "   190,\n",
              "   6390,\n",
              "   62481,\n",
              "   12095,\n",
              "   52989,\n",
              "   21883,\n",
              "   15,\n",
              "   3285,\n",
              "   519,\n",
              "   47148,\n",
              "   32881,\n",
              "   2],\n",
              "  'attention_mask': [1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1],\n",
              "  'start_positions': [27],\n",
              "  'end_positions': [27]},\n",
              " {'input_ids': [0,\n",
              "   12751,\n",
              "   14622,\n",
              "   9654,\n",
              "   52881,\n",
              "   101514,\n",
              "   3686,\n",
              "   68250,\n",
              "   10687,\n",
              "   32,\n",
              "   2,\n",
              "   2,\n",
              "   12751,\n",
              "   14622,\n",
              "   9654,\n",
              "   52881,\n",
              "   15,\n",
              "   33099,\n",
              "   3937,\n",
              "   31840,\n",
              "   2650,\n",
              "   6016,\n",
              "   12,\n",
              "   641,\n",
              "   10259,\n",
              "   79567,\n",
              "   16,\n",
              "   83623,\n",
              "   4575,\n",
              "   16891,\n",
              "   8850,\n",
              "   4875,\n",
              "   155837,\n",
              "   5055,\n",
              "   100789,\n",
              "   3686,\n",
              "   229613,\n",
              "   7016,\n",
              "   31107,\n",
              "   1962,\n",
              "   210073,\n",
              "   4,\n",
              "   209205,\n",
              "   203011,\n",
              "   5,\n",
              "   12751,\n",
              "   14622,\n",
              "   9654,\n",
              "   110705,\n",
              "   95818,\n",
              "   84909,\n",
              "   71176,\n",
              "   155012,\n",
              "   222181,\n",
              "   3153,\n",
              "   19924,\n",
              "   6343,\n",
              "   1629,\n",
              "   135089,\n",
              "   216740,\n",
              "   25432,\n",
              "   6001,\n",
              "   160764,\n",
              "   4167,\n",
              "   4,\n",
              "   188744,\n",
              "   2802,\n",
              "   215468,\n",
              "   131846,\n",
              "   4930,\n",
              "   19409,\n",
              "   12152,\n",
              "   5966,\n",
              "   43427,\n",
              "   4,\n",
              "   35576,\n",
              "   2650,\n",
              "   192647,\n",
              "   27756,\n",
              "   4,\n",
              "   119547,\n",
              "   12152,\n",
              "   73807,\n",
              "   49943,\n",
              "   4,\n",
              "   6,\n",
              "   199852,\n",
              "   49943,\n",
              "   6509,\n",
              "   73807,\n",
              "   4,\n",
              "   17984,\n",
              "   12391,\n",
              "   6736,\n",
              "   10753,\n",
              "   17708,\n",
              "   10370,\n",
              "   11449,\n",
              "   197266,\n",
              "   4,\n",
              "   8120,\n",
              "   17708,\n",
              "   114319,\n",
              "   74240,\n",
              "   3937,\n",
              "   8040,\n",
              "   33625,\n",
              "   4,\n",
              "   6,\n",
              "   8850,\n",
              "   2802,\n",
              "   6,\n",
              "   49943,\n",
              "   73221,\n",
              "   34987,\n",
              "   86143,\n",
              "   7065,\n",
              "   31107,\n",
              "   61253,\n",
              "   155837,\n",
              "   31324,\n",
              "   65808,\n",
              "   78675,\n",
              "   12928,\n",
              "   16891,\n",
              "   152711,\n",
              "   5,\n",
              "   84735,\n",
              "   14233,\n",
              "   2025,\n",
              "   144047,\n",
              "   17289,\n",
              "   175081,\n",
              "   72594,\n",
              "   5966,\n",
              "   27705,\n",
              "   80024,\n",
              "   938,\n",
              "   237590,\n",
              "   10106,\n",
              "   6149,\n",
              "   80334,\n",
              "   8182,\n",
              "   6110,\n",
              "   212933,\n",
              "   87262,\n",
              "   5,\n",
              "   224264,\n",
              "   7216,\n",
              "   133523,\n",
              "   12751,\n",
              "   6736,\n",
              "   2782,\n",
              "   6379,\n",
              "   138318,\n",
              "   110764,\n",
              "   10106,\n",
              "   47059,\n",
              "   18572,\n",
              "   7114,\n",
              "   192606,\n",
              "   4,\n",
              "   2690,\n",
              "   178870,\n",
              "   3770,\n",
              "   8911,\n",
              "   72594,\n",
              "   5966,\n",
              "   193154,\n",
              "   6,\n",
              "   183532,\n",
              "   13070,\n",
              "   25950,\n",
              "   197144,\n",
              "   6509,\n",
              "   6566,\n",
              "   7114,\n",
              "   192606,\n",
              "   235186,\n",
              "   33234,\n",
              "   5,\n",
              "   142113,\n",
              "   12751,\n",
              "   14622,\n",
              "   9654,\n",
              "   52881,\n",
              "   74,\n",
              "   188032,\n",
              "   15413,\n",
              "   10890,\n",
              "   13764,\n",
              "   2798,\n",
              "   13945,\n",
              "   3791,\n",
              "   35235,\n",
              "   4,\n",
              "   12359,\n",
              "   78251,\n",
              "   188514,\n",
              "   12784,\n",
              "   10370,\n",
              "   216652,\n",
              "   111372,\n",
              "   186483,\n",
              "   8120,\n",
              "   1962,\n",
              "   4551,\n",
              "   6161,\n",
              "   5944,\n",
              "   55880,\n",
              "   2913,\n",
              "   61995,\n",
              "   133523,\n",
              "   176864,\n",
              "   81532,\n",
              "   141909,\n",
              "   31142,\n",
              "   28108,\n",
              "   18232,\n",
              "   6,\n",
              "   183532,\n",
              "   87783,\n",
              "   4,\n",
              "   13128,\n",
              "   115706,\n",
              "   178870,\n",
              "   91211,\n",
              "   1629,\n",
              "   212933,\n",
              "   120159,\n",
              "   5,\n",
              "   8120,\n",
              "   1962,\n",
              "   4551,\n",
              "   6161,\n",
              "   5944,\n",
              "   55880,\n",
              "   2913,\n",
              "   12407,\n",
              "   207014,\n",
              "   209365,\n",
              "   111372,\n",
              "   2912,\n",
              "   4,\n",
              "   30419,\n",
              "   5,\n",
              "   10832,\n",
              "   5,\n",
              "   106,\n",
              "   83505,\n",
              "   237590,\n",
              "   8938,\n",
              "   4,\n",
              "   56248,\n",
              "   188032,\n",
              "   17984,\n",
              "   3769,\n",
              "   9313,\n",
              "   2025,\n",
              "   145600,\n",
              "   235186,\n",
              "   35097,\n",
              "   148687,\n",
              "   99768,\n",
              "   12009,\n",
              "   12359,\n",
              "   78251,\n",
              "   188514,\n",
              "   12784,\n",
              "   10370,\n",
              "   6345,\n",
              "   94339,\n",
              "   22152,\n",
              "   28108,\n",
              "   65479,\n",
              "   87262,\n",
              "   5,\n",
              "   14780,\n",
              "   207014,\n",
              "   209365,\n",
              "   8120,\n",
              "   1962,\n",
              "   4551,\n",
              "   6161,\n",
              "   5944,\n",
              "   55880,\n",
              "   2913,\n",
              "   4,\n",
              "   3219,\n",
              "   222181,\n",
              "   2025,\n",
              "   55241,\n",
              "   74750,\n",
              "   6110,\n",
              "   3219,\n",
              "   54512,\n",
              "   202287,\n",
              "   160985,\n",
              "   2912,\n",
              "   5,\n",
              "   12359,\n",
              "   78251,\n",
              "   30512,\n",
              "   10106,\n",
              "   12784,\n",
              "   10370,\n",
              "   6345,\n",
              "   8120,\n",
              "   1962,\n",
              "   4551,\n",
              "   6161,\n",
              "   5944,\n",
              "   55880,\n",
              "   2913,\n",
              "   4,\n",
              "   218380,\n",
              "   181271,\n",
              "   12152,\n",
              "   2025,\n",
              "   70778,\n",
              "   15,\n",
              "   16891,\n",
              "   5,\n",
              "   19520,\n",
              "   79318,\n",
              "   20,\n",
              "   201,\n",
              "   62155,\n",
              "   13128,\n",
              "   6,\n",
              "   2782,\n",
              "   162320,\n",
              "   70778,\n",
              "   67939,\n",
              "   15,\n",
              "   16891,\n",
              "   5,\n",
              "   19520,\n",
              "   5,\n",
              "   305,\n",
              "   83505,\n",
              "   237590,\n",
              "   3686,\n",
              "   16,\n",
              "   226795,\n",
              "   1629,\n",
              "   94339,\n",
              "   22152,\n",
              "   2802,\n",
              "   195291,\n",
              "   2802,\n",
              "   105977,\n",
              "   14233,\n",
              "   2025,\n",
              "   4875,\n",
              "   47128,\n",
              "   94200,\n",
              "   18806,\n",
              "   15377,\n",
              "   33099,\n",
              "   67336,\n",
              "   3219,\n",
              "   40582,\n",
              "   2798,\n",
              "   7827,\n",
              "   5055,\n",
              "   15,\n",
              "   10145,\n",
              "   16,\n",
              "   102080,\n",
              "   111372,\n",
              "   30346,\n",
              "   43616,\n",
              "   131846,\n",
              "   4930,\n",
              "   181271,\n",
              "   14233,\n",
              "   2025,\n",
              "   70778,\n",
              "   1995,\n",
              "   378,\n",
              "   6736,\n",
              "   17708,\n",
              "   2],\n",
              "  'attention_mask': [1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1],\n",
              "  'start_positions': [0],\n",
              "  'end_positions': [0]}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map__data(features):\n",
        "  data_ = {}\n",
        "  for k in features[0].keys():\n",
        "    data_[k] = [data_[k] for data_ in features]\n",
        "  return data_"
      ],
      "metadata": {
        "id": "N8UnJ9wzDWzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = map__data(train_dataset)\n",
        "val_df = map__data(val_dataset)"
      ],
      "metadata": {
        "id": "Fu1Km5mT2xiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEiPAZVXonb",
        "outputId": "b5e067dc-c24d-4849-ad49-673d62ffbeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFrh-kgXYKq4",
        "outputId": "8da74832-0fde-48ad-a59c-5eadf58862ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "def convert_df_todict(data):\n",
        "  # convert dataset to dictionary from dataframe\n",
        "  data_dict = Dataset.from_pandas(data)\n",
        "  return data_dict"
      ],
      "metadata": {
        "id": "mHfOllXQ_2-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = convert_df_todict(pd.DataFrame.from_dict(train_df,orient='index').transpose())\n",
        "val_loader = convert_df_todict(pd.DataFrame.from_dict(val_df,orient='index').transpose())"
      ],
      "metadata": {
        "id": "TKthMCXc2BD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke3a9tR5XXSf",
        "outputId": "8bad73b0-60d1-4d74-c827-5190e5aea09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 914\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dvvn_eoXdFe",
        "outputId": "f3ea9208-cad2-4032-b176-dfba45ace8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = TrainingArguments( output_dir = 'bert_best_model', overwrite_output_dir = True,\n",
        "                           evaluation_strategy = 'epoch', learning_rate = 0.0001, \n",
        "                           gradient_accumulation_steps = 8,\n",
        "                           per_device_train_batch_size = 4,\n",
        "                           per_device_eval_batch_size = 4,\n",
        "                           num_train_epochs = 4, weight_decay = 0.01,\n",
        "                           save_strategy = 'epoch', no_cuda = False,\n",
        "                           logging_strategy = 'steps')"
      ],
      "metadata": {
        "id": "-8AqhpRVZFRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30ANuvF9ZG28",
        "outputId": "2f8769f2-b2dc-4a95-daaa-410872b956e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_pin_memory=True,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_steps=None,\n",
              "evaluation_strategy=epoch,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=8,\n",
              "gradient_checkpointing=False,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=False,\n",
              "hub_strategy=every_save,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_inputs_for_metrics=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=0.0001,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=-1,\n",
              "log_level=passive,\n",
              "log_level_replica=passive,\n",
              "log_on_each_node=True,\n",
              "logging_dir=bert_best_model/runs/Oct30_08-54-11_1654d88399a9,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=steps,\n",
              "lr_scheduler_type=linear,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "no_cuda=False,\n",
              "num_train_epochs=4,\n",
              "optim=adamw_hf,\n",
              "output_dir=bert_best_model,\n",
              "overwrite_output_dir=True,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=4,\n",
              "per_device_train_batch_size=4,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=['tensorboard'],\n",
              "resume_from_checkpoint=None,\n",
              "run_name=bert_best_model,\n",
              "save_on_each_node=False,\n",
              "save_steps=500,\n",
              "save_strategy=epoch,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "sharded_ddp=[],\n",
              "skip_memory_metrics=True,\n",
              "tf32=None,\n",
              "torchdynamo=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.01,\n",
              "xpu_backend=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer = Trainer( model = model, args = params, train_dataset = train_loader, eval_dataset = val_loader,\n",
        "                   data_collator = data_collator, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "Gu1MA1QSVSAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Model Training**"
      ],
      "metadata": {
        "id": "nLqeyOqYuZXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start model training\n",
        "model_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4hz7NjywVVMU",
        "outputId": "ee86dbdd-130b-43c2-ebed-fe7491db62ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 914\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 112\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [112/112 05:11, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.718499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.611260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.861912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.155155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to bert_best_model/checkpoint-28\n",
            "Configuration saved in bert_best_model/checkpoint-28/config.json\n",
            "Model weights saved in bert_best_model/checkpoint-28/pytorch_model.bin\n",
            "tokenizer config file saved in bert_best_model/checkpoint-28/tokenizer_config.json\n",
            "Special tokens file saved in bert_best_model/checkpoint-28/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to bert_best_model/checkpoint-56\n",
            "Configuration saved in bert_best_model/checkpoint-56/config.json\n",
            "Model weights saved in bert_best_model/checkpoint-56/pytorch_model.bin\n",
            "tokenizer config file saved in bert_best_model/checkpoint-56/tokenizer_config.json\n",
            "Special tokens file saved in bert_best_model/checkpoint-56/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to bert_best_model/checkpoint-84\n",
            "Configuration saved in bert_best_model/checkpoint-84/config.json\n",
            "Model weights saved in bert_best_model/checkpoint-84/pytorch_model.bin\n",
            "tokenizer config file saved in bert_best_model/checkpoint-84/tokenizer_config.json\n",
            "Special tokens file saved in bert_best_model/checkpoint-84/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 4\n",
            "Saving model checkpoint to bert_best_model/checkpoint-112\n",
            "Configuration saved in bert_best_model/checkpoint-112/config.json\n",
            "Model weights saved in bert_best_model/checkpoint-112/pytorch_model.bin\n",
            "tokenizer config file saved in bert_best_model/checkpoint-112/tokenizer_config.json\n",
            "Special tokens file saved in bert_best_model/checkpoint-112/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=112, training_loss=1.040583883013044, metrics={'train_runtime': 313.6646, 'train_samples_per_second': 11.656, 'train_steps_per_second': 0.357, 'total_flos': 712948200754176.0, 'train_loss': 1.040583883013044, 'epoch': 3.98})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Evaluation phase**"
      ],
      "metadata": {
        "id": "5-iREpiUuh0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make test data tokenization class\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TestDataTokenization(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: AutoTokenizer, text_max_length: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.text_max_length = text_max_length\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        \n",
        "        tokenized_text = tokenizer(\n",
        "        data_row[\"question\" if padding_style  else \"context\"], data_row[\"context\" if padding_style  else \"question\"],\n",
        "        padding = \"max_length\", truncation = \"only_second\" if padding_style  else \"only_first\",\n",
        "        max_length = self.text_max_length, return_attention_mask=True,\n",
        "        add_special_tokens=True, return_offsets_mapping=True, return_tensors=None)\n",
        "        # extract sequence id\n",
        "        tokens_style = tokenized_text.sequence_ids()\n",
        "        c_id = 1 if padding_style else 0\n",
        "        # extract all offset mapping \n",
        "        tokenized_text[\"offset_mapping\"] = [\n",
        "            (offset if tokens_style[value] == c_id else None)\n",
        "            for value, offset in enumerate(tokenized_text[\"offset_mapping\"])]\n",
        "        \n",
        "        return dict(\n",
        "            input_ids = tokenized_text['input_ids'],\n",
        "            attention_mask = tokenized_text['attention_mask'],\n",
        "            offset_mapping = tokenized_text['offset_mapping']\n",
        "            )"
      ],
      "metadata": {
        "id": "NtdaVqTMZ6px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = list(TestDataTokenization(test_data, tokenizer, 384))"
      ],
      "metadata": {
        "id": "hiCJjQwE1X0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = map__data(test_dataset)"
      ],
      "metadata": {
        "id": "f-ipe1dm1X0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be7b919-75c7-4421-a938-e3f75a6cacef",
        "id": "HKbFfrjw1X0J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "test_loader = convert_df_todict(pd.DataFrame.from_dict(test_df,orient='index').transpose())"
      ],
      "metadata": {
        "id": "zLva4Uq-1X0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader_model = test_loader.map(remove_columns=[ 'offset_mapping'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fa781ac5d2b345afabfc9277e6f39189",
            "35ff0edb434b40e98186e44c483e7b6e",
            "e02fe8bb50e94875bc903ce654c8e0c2",
            "5bbd12de93544fc182b56daa0f670d91",
            "9666eda3d696465cb85df4493c70c6b4",
            "569a13b6fb4e4ac8b6f5d03ca28a8a91",
            "e4c5cab7829c4bd0a537ae9b5a274af3",
            "061cfb6d04bb4322a2172a8ab23f40cf",
            "9881bd0dcac747c088bb6aba560eb512",
            "8aad8b847cf94bbeae33fc812f924b49",
            "6cddf42548094be380523ea65ae9d792"
          ]
        },
        "outputId": "2b336de4-a7e2-4560-a787-6f5950573b56",
        "id": "ak5Goa031X0P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa781ac5d2b345afabfc9277e6f39189"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Model predictions**"
      ],
      "metadata": {
        "id": "ZrY9-zn2X72z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = model_trainer.predict(test_loader_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "tp4V1N4PyOis",
        "outputId": "0f13c745-c5f5-4c6b-c966-d356c55babf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('bert_predictions.pickle', 'wb') as handle:\n",
        "    pickle.dump(model_predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "lDqnhFkozSpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bert_predictions.pickle', 'rb') as handle:\n",
        "    model_predictions = pickle.load(handle)"
      ],
      "metadata": {
        "id": "OC9Zh3srAHaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_prediction_seq, end_prediction_seq = model_predictions.predictions"
      ],
      "metadata": {
        "id": "qYKfvITw7nhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_predicted_answers(raw_data, tokenizer_features, start_prediction_seq, end_prediction_seq, top, target_length):\n",
        "    # apply processing on generated predictions of model\n",
        "    pred_ans_list = []\n",
        "    # iterate over test data instances\n",
        "    for id, data_row in enumerate(raw_data):\n",
        "      context = data_row[\"context\"]\n",
        "      m_value = None \n",
        "      best_predicted_answers = []\n",
        "      start_prediction = start_prediction_seq[id]\n",
        "      end_prediction = end_prediction_seq[id]\n",
        "      offset_tokens = tokenizer_features[id][\"offset_mapping\"]\n",
        "      start_index = tokenizer_features[id][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "      # find the minimum null value in feature vector\n",
        "      feature_min_value = start_prediction[start_index] + end_prediction[start_index]\n",
        "      if m_value is None or m_value < feature_min_value:\n",
        "        m_value = feature_min_value\n",
        "      start_prediction_seq_idx = np.argsort(start_prediction)[-1 : -top - 1 : -1].tolist()\n",
        "      end_prediction_seq_idx = np.argsort(end_prediction)[-1 : -top - 1 : -1].tolist()\n",
        "      for start_prediction_id in start_prediction_seq_idx:\n",
        "        # iterate over all selected predicted vlaues of each instance end ids\n",
        "        for end_prediction_id in end_prediction_seq_idx:\n",
        "            if (start_prediction_id >= len(offset_tokens) or end_prediction_id >= len(offset_tokens)\n",
        "                or offset_tokens[end_prediction_id] is None or offset_tokens[start_prediction_id] is None):\n",
        "              continue\n",
        "            if end_prediction_id - start_prediction_id + 1 > target_length or end_prediction_id < start_prediction_id:\n",
        "              continue\n",
        "            ans_start_position = offset_tokens[start_prediction_id][0]\n",
        "            ans_end_position = offset_tokens[end_prediction_id][1]\n",
        "            ans_prob = start_prediction[start_prediction_id] + end_prediction[end_prediction_id]\n",
        "            result_ans_dict = { \"pred_prob\": ans_prob, \"pred_answer_text\": context[ans_start_position: ans_end_position]}\n",
        "            best_predicted_answers.append(result_ans_dict)\n",
        "      if len(best_predicted_answers) <= 0:\n",
        "        # assign probability 0 and return predicted answer as empty string\n",
        "        predicted_answer = {\"pred_answer_text\": \"\", \"pred_prob\": 0.0}\n",
        "      else:\n",
        "        predicted_answer = sorted(best_predicted_answers, key=lambda y: y[\"pred_prob\"], reverse=True)[0]  \n",
        "      pred_ans_list.append(predicted_answer[\"pred_answer_text\"])\n",
        "    # return predictions list\n",
        "    return pred_ans_list"
      ],
      "metadata": {
        "id": "kzo6UY39Ou2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_list = extract_predicted_answers(convert_df_todict(test_data), test_loader, start_prediction_seq, end_prediction_seq, 15, 30)"
      ],
      "metadata": {
        "id": "TfnouxqNQXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to evaluate bleu scores of predicted answeres vs original answers\n",
        "def evaluate_blue_score(actual, prediction):\n",
        "  results = dict()\n",
        "  bleu_score1 = 0\n",
        "  bleu_score2 = 0\n",
        "  bleu_score3 = 0\n",
        "  bleu_score4 = 0\n",
        "  if len(actual) == len(prediction):\n",
        "    # iterate over all predicitons\n",
        "    for i in range(len(actual)):\n",
        "      if prediction == \"\":\n",
        "        return 0,0,0,0\n",
        "      actual_tokenized = list(map(lambda x: indic_tokenize.trivial_tokenize(x), actual[i]))\n",
        "      pred_tokenized = indic_tokenize.trivial_tokenize(prediction[i])\n",
        "      chencherry = SmoothingFunction()\n",
        "      # nltk functions to calculate bleu1, bleu2, bleu3 and bleu4 score\n",
        "      bleu_1 = sentence_bleu(actual_tokenized, pred_tokenized, weights=(1, 0, 0, 0), smoothing_function=chencherry.method2)\n",
        "      bleu_2 = sentence_bleu(actual_tokenized, pred_tokenized, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method2)\n",
        "      bleu_3 = sentence_bleu(actual_tokenized, pred_tokenized, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method2)\n",
        "      bleu_4 = sentence_bleu(actual_tokenized, pred_tokenized, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method2)\n",
        "      # add up scores of each instance\n",
        "      bleu_score1 +=bleu_1\n",
        "      bleu_score2 +=bleu_2\n",
        "      bleu_score3 +=bleu_3\n",
        "      bleu_score4 +=bleu_4\n",
        "    # convert decimale values  bleu scores to percentage\n",
        "    results[\"bleu_1\"] = [round(bleu_score1 / len(actual) * 100, 2)]\n",
        "    results[\"bleu_2\"] = [round(bleu_score2 / len(actual) * 100, 2)]\n",
        "    results[\"bleu_3\"] = [round(bleu_score3 / len(actual) * 100, 2)]\n",
        "    results[\"bleu_4\"] = [round(bleu_score4 / len(actual) * 100, 2)]\n",
        "    # return total evaluated results\n",
        "    return results\n",
        "  else:\n",
        "    print(\"Error: Actual values and predictions are not of same length....\")\n"
      ],
      "metadata": {
        "id": "Jm70imXB8KVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP3T87P18QuU",
        "outputId": "1edcce64-d14b-408f-c062-23d54ed626c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_answers = list(test_data['answer_text'].values)"
      ],
      "metadata": {
        "id": "xnaJpM368V3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_answers = [i.strip(\" \").strip(\"\\n\") for i in predictions_list]"
      ],
      "metadata": {
        "id": "auC4ubaY8ij4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate bleu scores\n",
        "actual_answers = [[i] for i in actual_answers]\n",
        "ans_results = evaluate_blue_score(actual_answers, pred_answers)\n",
        "pd.DataFrame(ans_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "MWixIMod8mrD",
        "outputId": "0f897953-0c58-4066-ac36-298d588367b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   bleu_1  bleu_2  bleu_3  bleu_4\n",
              "0   45.51   41.48   37.79   34.76"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac1727d6-598e-4685-b5c8-2bcc46e7dafc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bleu_1</th>\n",
              "      <th>bleu_2</th>\n",
              "      <th>bleu_3</th>\n",
              "      <th>bleu_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45.51</td>\n",
              "      <td>41.48</td>\n",
              "      <td>37.79</td>\n",
              "      <td>34.76</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac1727d6-598e-4685-b5c8-2bcc46e7dafc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac1727d6-598e-4685-b5c8-2bcc46e7dafc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac1727d6-598e-4685-b5c8-2bcc46e7dafc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(actual_answers)):\n",
        "  print(\"\\n----------------------------------\\n\")\n",
        "  print(\"Actual answers: \",actual_answers[i][0])\n",
        "  print(\"Predicted answer: \",pred_answers[i])\n",
        "  try:\n",
        "    print(\"Translated Answer: \",GoogleTranslator(source='auto', target='en').translate(pred_answers[i]))\n",
        "  except:\n",
        "    print(\"Translated Answer: \",pred_answers[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjh3c7Zd88dm",
        "outputId": "708988bf-51bc-4b64-ea35-8256877df628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  16 9, 250 वर्ग मील\n",
            "Predicted answer:  16 9, 250 वर्ग मील\n",
            "Translated Answer:  169,250 square miles\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  नाथूराम गोडसे\n",
            "Predicted answer:  नाथूराम गोडसे\n",
            "Translated Answer:  Nathuram Godse\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  स्त्री-रोग विशेषज्ञ\n",
            "Predicted answer:  स्त्रीरोगविज्ञान\n",
            "Translated Answer:  gynecology\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  लक्ष्मी\n",
            "Predicted answer:  लक्ष्मी\n",
            "Translated Answer:  Laxmi\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  जेद्दाह से १९कि.मी उत्तर में स्थित\n",
            "Predicted answer:  सऊदी अरब के शहर जेद्दाह\n",
            "Translated Answer:  Saudi Arabian city Jeddah\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  चीन\n",
            "Predicted answer:  चीन\n",
            "Translated Answer:  China\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  चार्ल्स बैबेज\n",
            "Predicted answer:  चार्ल्स बैबेज\n",
            "Translated Answer:  Charles Babbage\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  331 ई.पू.\n",
            "Predicted answer:  331 ई.पू.\n",
            "Translated Answer:  331 BC\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1989\n",
            "Predicted answer:  1989\n",
            "Translated Answer:  1989\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  मोरोक्कन कयानेडेली\n",
            "Predicted answer:  मोरोक्कन कयानेडेली\n",
            "Translated Answer:  Moroccan Kayanedeli\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  रज्जब\n",
            "Predicted answer:  वर्ष में बारह मास\n",
            "Translated Answer:  twelve months in a year\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  287 ई.पू.\n",
            "Predicted answer:  287 ई.पू.\n",
            "Translated Answer:  287 BC\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  18\n",
            "Predicted answer:  रासायनिक\n",
            "Translated Answer:  chemical\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  स्विटजरलैंड के जेनेवा शहर में\n",
            "Predicted answer:  जेनेवा\n",
            "Translated Answer:  Geneva\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  रोमानियाई\n",
            "Predicted answer:  रोमानियाई\n",
            "Translated Answer:  Romanian\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1968\n",
            "Predicted answer:  1 जून 1968\n",
            "Translated Answer:  1 June 1968\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1986\n",
            "Predicted answer:  9 मई 1986\n",
            "Translated Answer:  9 May 1986\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  हिन्दु\n",
            "Predicted answer:  हिन्दू\n",
            "Translated Answer:  Hindu\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  पांड्यों\n",
            "Predicted answer:  लिंगम\n",
            "Translated Answer:  Lingam\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ७३८.८ वर्ग किलोमीटर\n",
            "Predicted answer:  ३२ किमी.\n",
            "Translated Answer:  32 km.\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  कोलकाता\n",
            "Predicted answer:  पश्चिम बंगाल\n",
            "Translated Answer:  West Bengal\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ऑक्सफ़ोर्ड\n",
            "Predicted answer:  ऑक्सफ़ोर्ड\n",
            "Translated Answer:  oxford\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  जर्मनी के स्टटगार्ट\n",
            "Predicted answer:  जर्मन\n",
            "Translated Answer:  German\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  अहमद शाह अब्दाली\n",
            "Predicted answer:  अहमद शाह अब्दाली\n",
            "Translated Answer:  Ahmad Shah Abdali\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  मंदारिन\n",
            "Predicted answer:  ताइवान या ताईवान (चीनी: 台灣) पूर्व एशिया में स्थित एक द्वीप है।\n",
            "Translated Answer:  Taiwan or Taiwan (Chinese: ) is an island located in East Asia.\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  राजा शुद्धोधन\n",
            "Predicted answer:  महामाया\n",
            "Translated Answer:  Mahamaya\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  पाकिस्तान तहरीक-ए-इंसाफ\n",
            "Predicted answer:  पाकिस्तानी राजनीतिज्ञ\n",
            "Translated Answer:  Pakistani politician\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  भारत के मध्य प्रदेश राज्य का एक प्रमुख शहर है जो क्षिप्रा नदी के किनारे बसा है\n",
            "Predicted answer:  मध्य प्रदेश\n",
            "Translated Answer:  Madhya Pradesh\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  563 ईसा पूर्व\n",
            "Predicted answer:  563 ईसा पूर्व\n",
            "Translated Answer:  563 BC\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1852\n",
            "Predicted answer:  21 जून, 1852\n",
            "Translated Answer:  June 21, 1852\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  एमा चारलॉट डुएरे वॉटसन\n",
            "Predicted answer:  चारलॉट डुएरे वॉटसन\n",
            "Translated Answer:  Charlotte Duere Watson\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1905\n",
            "Predicted answer:  1930\n",
            "Translated Answer:  1930\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  संयुक्त राष्ट्र के अर्मोंक, न्यू यॉर्क में\n",
            "Predicted answer:  अर्मोंक, न्यू यॉर्क\n",
            "Translated Answer:  Armonk, New York\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  निवृत्तिनाथ\n",
            "Predicted answer:  रुक्मिणी बाई\n",
            "Translated Answer:  Rukmini Bai\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  सिर\n",
            "Predicted answer:  सिर\n",
            "Translated Answer:  head\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  २० फीट से २५ फुट तक\n",
            "Predicted answer:  २० फीट\n",
            "Translated Answer:  20 feet\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  12 नवम्बर 2015\n",
            "Predicted answer:  12 नवम्बर 2015\n",
            "Translated Answer:  12 November 2015\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  होरडियम डिस्टिन\n",
            "Predicted answer:  यव\n",
            "Translated Answer:  yaw\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  त्रिपोली\n",
            "Predicted answer:  मिस्र\n",
            "Translated Answer:  Egypt\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  सात हिंदुस्तानी\n",
            "Predicted answer:  बॉलीवुड\n",
            "Translated Answer:  Bollywood\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  अक्टूबर 1992\n",
            "Predicted answer:  अक्टूबर 1992\n",
            "Translated Answer:  October 1992\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  अनवर अहमद\n",
            "Predicted answer:  केशव शंकर पिल्लै जिन्हें ‘‘शंकर’’\n",
            "Translated Answer:  Keshav Shankar Pillai known as \"Shankar\"\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  १,४८४ वर्ग किलोमीटर\n",
            "Predicted answer:  १,४८४ वर्ग किलोमीटर\n",
            "Translated Answer:  1,484 square kilometers\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1312 किलोमीटर\n",
            "Predicted answer:  1312 किलोमीटर\n",
            "Translated Answer:  1312 kms\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  १९५४ ई.\n",
            "Predicted answer:  लेनिन शान्ति पुरस्कार\n",
            "Translated Answer:  Lenin Peace Prize\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  मार्च 14, 1965\n",
            "Predicted answer:  मार्च 14, 1965\n",
            "Translated Answer:  March 14, 1965\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  कोलेस्ट्रॉल\n",
            "Predicted answer:  बायोटेक्नोलॉजी\n",
            "Translated Answer:  biotechnology\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  15 अक्टूबर 1854\n",
            "Predicted answer:  46\n",
            "Translated Answer:  46\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ४२,६८५\n",
            "Predicted answer:  पांचवें भाग\n",
            "Translated Answer:  fifth part\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  देवेन्द्रनाथ ठाकुर\n",
            "Predicted answer:  देवेन्द्रनाथ ठाकुर\n",
            "Translated Answer:  devendranath thakur\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  १५ जून १९०२\n",
            "Predicted answer:  जर्मनी\n",
            "Translated Answer:  Germany\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  कन्नड़\n",
            "Predicted answer:  कन्नड़\n",
            "Translated Answer:  Kannada\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  लातवियाई भाषा\n",
            "Predicted answer:  लातवियाई\n",
            "Translated Answer:  Latvian\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  कैवेंडिश प्रोफ़ेसर\n",
            "Predicted answer:  भौतिक विज्ञानी\n",
            "Translated Answer:  physicist\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  भारतीय संविधान के भाग ४ के अनुच्छेद ४४ में\n",
            "Predicted answer:  धर्मनिरपेक्ष\n",
            "Translated Answer:  secular\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  होली\n",
            "Predicted answer:  होली\n",
            "Translated Answer:  Holi\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  नेपोलियन बोनापार्ट\n",
            "Predicted answer:  नेपोलियन बोनापार्ट\n",
            "Translated Answer:  Napoleon Bonaparte\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  भोजदेव\n",
            "Predicted answer:  राजा भोज\n",
            "Translated Answer:  Raja Bhoj\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  जोया अख्तर\n",
            "Predicted answer:  जोया अख्तर\n",
            "Translated Answer:  Zoya Akhtar\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ककुर्बितासिए\n",
            "Predicted answer:  जीनस ककुर्बिता\n",
            "Translated Answer:  genus Cucurbita\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  रावल जैसल\n",
            "Predicted answer:  ।\n",
            "Translated Answer:  None\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  अमेरिकी राष्ट्रीय मानक संस्थान\n",
            "Predicted answer:  डेनिस रिची\n",
            "Translated Answer:  Dennis Ritchie\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  18 वर्ष\n",
            "Predicted answer:  18\n",
            "Translated Answer:  18\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1998\n",
            "Predicted answer:  1998\n",
            "Translated Answer:  1998\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  22 अक्टूबर 1964\n",
            "Predicted answer:  22 अक्टूबर 1964\n",
            "Translated Answer:  22 October 1964\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  माउंट झूजी\n",
            "Predicted answer:  न्यु ताइपे\n",
            "Translated Answer:  New Taipei\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  फिलिप्पा\n",
            "Predicted answer:  अल्स्टर के अर्ल\n",
            "Translated Answer:  Earl of Ulster\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  हारमैटन\n",
            "Predicted answer:  हारमैटन\n",
            "Translated Answer:  harmton\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  97,776 वर्ग किलोमीटर\n",
            "Predicted answer:  97,776 वर्ग किलोमीटर\n",
            "Translated Answer:  97,776 square kilometer\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  400 ई॰पू॰ 100 ई॰ सन् के बीच\n",
            "Predicted answer:  400 ई॰पू॰ 100 ई॰ सन्\n",
            "Translated Answer:  400 BC to 100 AD\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  २५ दिसम्बर १९४९\n",
            "Predicted answer:  २५ दिसम्बर १९४९\n",
            "Translated Answer:  25 December 1949\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  अप्रैल १९९७\n",
            "Predicted answer:  अप्रैल १९९७\n",
            "Translated Answer:  April 1997\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  किसन बाबूराव हजारे\n",
            "Predicted answer:  बाबूराव हजारे\n",
            "Translated Answer:  Baburao Hazare\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  15 अप्रैल 1469\n",
            "Predicted answer:  15 अप्रैल 1469\n",
            "Translated Answer:  15 April 1469\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  57 वर्ग किलोमीटर\n",
            "Predicted answer:  1.35 लाख\n",
            "Translated Answer:  1.35 lakh\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  पनामा शहर\n",
            "Predicted answer:  पनामा शहर\n",
            "Translated Answer:  panama city\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  इब्न मजाह\n",
            "Predicted answer:  कुतुब अल-सित्ताह\n",
            "Translated Answer:  Qutb al-Sittah\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1453\n",
            "Predicted answer:  27 ई.पू.\n",
            "Translated Answer:  27 BC\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  १३ मई १९१८\n",
            "Predicted answer:  १३ मई १९१८ मद्रास प्रेसीडेंसी, ब्रिटिश भारत\n",
            "Translated Answer:  13 May 1918 Madras Presidency, British India\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  महाराष्ट्र के कोंकण क्षेत्र में एक गांव है, गागोदा\n",
            "Predicted answer:  11 सितम्बर 1895\n",
            "Translated Answer:  11 September 1895\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  1773\n",
            "Predicted answer:  1773\n",
            "Translated Answer:  1773\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ३० से ज्यादा सदस्य\n",
            "Predicted answer:  प्रथम संसद\n",
            "Translated Answer:  first parliament\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  केविन सिस्ट्रॉम और माइक क्रेगर\n",
            "Predicted answer:  केविन सिस्ट्रॉम\n",
            "Translated Answer:  Kevin Systrom\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  महाराजा वल्लभ सेन\n",
            "Predicted answer:  शूरसेन\n",
            "Translated Answer:  shursen\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  इस्लामी\n",
            "Predicted answer:  मुसलमानों\n",
            "Translated Answer:  Muslims\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  ७४,१२२ वर्ग मील\n",
            "Predicted answer:  ७४,१२२ वर्ग मील\n",
            "Translated Answer:  74,122 square miles\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  दुबई\n",
            "Predicted answer:  दुनिया\n",
            "Translated Answer:  World\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  १४ सितम्बर\n",
            "Predicted answer:  १४ सितम्बर\n",
            "Translated Answer:  14 september\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  Chilaka Gorinka\n",
            "Predicted answer:  विद्रोही\n",
            "Translated Answer:  the rebels\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  जुलाई 1947\n",
            "Predicted answer:  १९४७\n",
            "Translated Answer:  १९४७\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  पण्डित दीनदयाल उपाध्याय जंक्शन रेलवे स्टेशन\n",
            "Predicted answer:  दीनदयाल उपाध्याय\n",
            "Translated Answer:  Deen Dayal Upadhyay\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  45\n",
            "Predicted answer:  42\n",
            "Translated Answer:  42\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  मुंबई, महाराष्ट्र\n",
            "Predicted answer:  मुंबई, महाराष्ट्र\n",
            "Translated Answer:  Mumbai, Maharashtra\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  जोस व्हीडन को क्रिस टेर्रियो के साथ फिल्म की पटकथा लेखन का श्रेय मिला\n",
            "Predicted answer:  जॉन बर्ग और जॉफ जोंस\n",
            "Translated Answer:  Jon Berg and Jeff Jones\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  स्टीवन पॉल \"स्टीव\" जॉब्स\n",
            "Predicted answer:  स्टीवन पॉल \"स्टीव\" जॉब्स\n",
            "Translated Answer:  Steven Paul \"Steve\" Jobs\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  दो\n",
            "Predicted answer:  दो\n",
            "Translated Answer:  Two\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  आँधी और तूफान\n",
            "Predicted answer:  हमारी विरासत\n",
            "Translated Answer:  our heritage\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  उत्तरी\n",
            "Predicted answer:  उत्तरी\n",
            "Translated Answer:  northern\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  वर्ष 2010-11\n",
            "Predicted answer:  2010-11\n",
            "Translated Answer:  2010-11\n",
            "\n",
            "----------------------------------\n",
            "\n",
            "Actual answers:  नरेन्द्र दामोदरदास मोदी\n",
            "Predicted answer:  दामोदरदास मोदी\n",
            "Translated Answer:  Damodardas Modi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiApSaNKIMi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}